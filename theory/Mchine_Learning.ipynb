{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd317be",
   "metadata": {},
   "source": [
    "MACHINE LEARNING\n",
    "=================\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that gives computers the ability to learn from data and improve \n",
    "their performance on a task without being explicitly programmed for every specific case.\n",
    "\n",
    "How does it work?\n",
    "==================\n",
    "\n",
    "Input data: You give the model a dataset with features (like height, weight, age) and sometimes labels (like “has disease” or “does not have disease”).\n",
    "\n",
    "Training: The model analyzes the data to learn relationships between features and outcomes.\n",
    "\n",
    "Prediction: After training, you can give new data, and the model can predict or classify it based on what it learned.\n",
    "\n",
    "\n",
    "MACHINE LEARNING VS TRADITIONAL PROGRAMMING\n",
    "=============================================\n",
    "\n",
    "\n",
    "1. Traditional Programming\n",
    "\n",
    "You write explicit instructions (code) that tell the computer exactly what to do.\n",
    "\n",
    "You define the rules, logic, and steps to solve a problem.\n",
    "\n",
    "Input data goes in, and the program follows the written steps to produce output.\n",
    "\n",
    "\n",
    "2. Machine Learning\n",
    "\n",
    "Instead of explicitly programming rules, you give data to a model.\n",
    "\n",
    "The model learns patterns or rules from the data automatically.\n",
    "\n",
    "You provide inputs and expected outputs (in supervised learning), and the ML model figures out the relationship.\n",
    "\n",
    "After training, the model can make predictions or decisions on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5735a8f",
   "metadata": {},
   "source": [
    "Machine Learning\n",
    "================\n",
    "\n",
    "ML is teaching computers to learn from data\n",
    "(just like humans learn from experience)\n",
    "\n",
    "Here we train based on a dataset\n",
    "\n",
    "Machine Learning are of three types\n",
    "===================================\n",
    "\n",
    "1) Supervised Learning\n",
    "2) Unsupervised Learning\n",
    "3) Reinforcement Learning\n",
    "\n",
    "\n",
    "Supervised Learning\n",
    "===================\n",
    "\n",
    "The given dataset having both input features and result\n",
    "\n",
    "Supervised Learning has two types:\n",
    "=================================\n",
    "\n",
    "Classification algorithms:\n",
    "=========================\n",
    ">>KNN (Using distance formula)\n",
    ">>Naive Bayes (Bayes theorem >> probability)\n",
    ">>SVM (Support Vector Machines)\n",
    ">>Decision Tree\n",
    ">>Random Forest\n",
    "\n",
    "\n",
    "Regression Algorithms:\n",
    "=====================\n",
    "Used to predict a numerical and continous values\n",
    "\n",
    "Algorithms\n",
    "==========\n",
    "\n",
    "1) Linear Regression\n",
    "2) Multiple linear regression\n",
    "3) Polynomial regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebf1c1",
   "metadata": {},
   "source": [
    "KNN Algorithm Steps\n",
    "===================\n",
    "\n",
    "\n",
    "\n",
    "Step_1: Select the number of K neighbours\n",
    "\n",
    "Step_2: Calculate the euclidean distance of the new value(x1,y1) from all the available datapoints\n",
    "\n",
    "Step_3: Take the K (given values in Step_1) nearest neighbours as per the calculated distance\n",
    "\n",
    "Step_4: Among these K neighbours selected count the number of datapoints in each category (count of pass and fail)\n",
    "\n",
    "Step_5: Assign the new datapoint to that category for which has the maximum count\n",
    "\n",
    "Step_6: Model has been created\n",
    "\n",
    "Euclidean distance >>> sqrt((x_i-x_new)^2 + (y_i-y_new)^2)\n",
    "\n",
    "(x_i,y_i) coordinates of points in the graph (existing student mark and attendacne)\n",
    "(x_new,y_new) coordinates of the new point which needs to be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a385997",
   "metadata": {},
   "source": [
    "OVERFITTING\n",
    "===========\n",
    "\n",
    "If we use 100% of data for training and test the model with data from the same data . This scenario is called Overfitting .Here the model will be having 100% accuracy. This should not happen because no model will ever be 100% accurate\n",
    "\n",
    "\n",
    "\n",
    "UNDERFITTING\n",
    "============\n",
    "\n",
    "In this case we use 30% of the dataset as training data and the remaining 70% data as the test data. This scenario is called Underfitting.In such case\n",
    "the model will be highly inaccurate because of very little training data\n",
    "\n",
    "\n",
    "In order to avaoid both underfitting and Overfitting we should always use 70%-80% data of the dataset as the training data and the remaining 30%-20% data as the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7260ee",
   "metadata": {},
   "source": [
    ".fit()\n",
    "======\n",
    "\n",
    "--------------------------------------------------\n",
    "|from sklearn.preprocessing import StandardScaler|\n",
    "|# We have to create a object for StandardScaler |\n",
    "|                                                |\n",
    "|scaler=StandardScaler()                         |\n",
    "|scaler.fit(X_train)                             |\n",
    "|X_train=scaler.transform(X_train)               |\n",
    "|X_test=scaler.transform(X_test)                 |\n",
    "|                                                |\n",
    " -------------------------------------------------\n",
    "\n",
    "When you call scaler.fit(X_train), it calculates the mean and standard deviation for each feature (column) in X_train.\n",
    "                                                     ----     ------------------\n",
    "These statistics are stored inside the scaler object.\n",
    "\n",
    "Later, when you call .transform() on data, it uses those stored mean and std values to scale the data (subtract mean, divide by std).\n",
    "\n",
    "So, .fit() means \"look at the data and figure out the numbers needed to transform it\" (like mean and std dev for scaling, or coefficients in a regression model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce217f",
   "metadata": {},
   "source": [
    "WHY WE DONT USE .fit() ON TEST DATA\n",
    "===================================\n",
    "\n",
    "The main reason: Avoid data leakage\n",
    "\n",
    "When you call .fit() on data, you’re telling the scaler to learn the parameters (like mean and standard deviation) from that data.\n",
    "\n",
    "If you fit the scaler on the test data, you’re allowing the model to “peek” at the test data statistics.\n",
    "\n",
    "This means the model indirectly gains knowledge about the test data before making predictions, which breaks the principle of keeping test data unseen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd387a",
   "metadata": {},
   "source": [
    ".transform()\n",
    "============\n",
    "\n",
    "When you use StandardScaler, the .transform() method scales each feature using this exact formula:\n",
    "----------------\n",
    "|z=(x-mean)/std|\n",
    "----------------\n",
    "x = original value for a feature\n",
    "\n",
    "mean = mean of that feature calculated from the training data during .fit()\n",
    "\n",
    "std = standard deviation of that feature from the training data during .fit()\n",
    "\n",
    "z = the scaled (standardized) value\n",
    "\n",
    "This process centers the data around 0 (mean becomes 0) and scales it so the variance is 1."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
